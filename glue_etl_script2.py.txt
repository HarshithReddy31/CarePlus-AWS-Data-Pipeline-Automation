import sys
from awsglue.transforms import *
from awsglue.utils import getResolvedOptions
from pyspark.context import SparkContext
from awsglue.context import GlueContext
from awsglue.job import Job
from awsglue.gluetypes import *
from awsgluedq.transforms import EvaluateDataQuality
from awsglue import DynamicFrame
import re

# âœ… Corrected argument parsing
args = getResolvedOptions(sys.argv, ['JOB_NAME', 'input_file_path'])

sc = SparkContext()
glueContext = GlueContext(sc)
spark = glueContext.spark_session
job = Job(glueContext)
job.init(args['JOB_NAME'], args)

# âœ… Use this variable for your input S3 path
input_path = args['input_file_path']

# ---------------------- CUSTOM FUNCTIONS ----------------------

def _find_null_fields(ctx, schema, path, output, nullStringSet, nullIntegerSet, frame):
    if isinstance(schema, StructType):
        for field in schema:
            new_path = path + "." if path != "" else path
            output = _find_null_fields(ctx, field.dataType, new_path + field.name, output, nullStringSet, nullIntegerSet, frame)
    elif isinstance(schema, ArrayType):
        if isinstance(schema.elementType, StructType):
            output = _find_null_fields(ctx, schema.elementType, path, output, nullStringSet, nullIntegerSet, frame)
    elif isinstance(schema, NullType):
        output.append(path)
    else:
        x, distinct_set = frame.toDF(), set()
        for i in x.select(path).distinct().collect():
            distinct_ = i[path.split('.')[-1]]
            if isinstance(distinct_, list):
                distinct_set |= set([item.strip() if isinstance(item, str) else item for item in distinct_])
            elif isinstance(distinct_, str):
                distinct_set.add(distinct_.strip())
            else:
                distinct_set.add(distinct_)
        if isinstance(schema, StringType):
            if distinct_set.issubset(nullStringSet):
                output.append(path)
        elif isinstance(schema, IntegerType) or isinstance(schema, LongType) or isinstance(schema, DoubleType):
            if distinct_set.issubset(nullIntegerSet):
                output.append(path)
    return output

def drop_nulls(glueContext, frame, nullStringSet, nullIntegerSet, transformation_ctx) -> DynamicFrame:
    nullColumns = _find_null_fields(frame.glue_ctx, frame.schema(), "", [], nullStringSet, nullIntegerSet, frame)
    return DropFields.apply(frame=frame, paths=nullColumns, transformation_ctx=transformation_ctx)

def sparkSqlQuery(glueContext, query, mapping, transformation_ctx) -> DynamicFrame:
    for alias, frame in mapping.items():
        frame.toDF().createOrReplaceTempView(alias)
    result = spark.sql(query)
    return DynamicFrame.fromDF(result, glueContext, transformation_ctx)

# ---------------------- ETL LOGIC ----------------------

DEFAULT_DATA_QUALITY_RULESET = """
    Rules = [
        ColumnCount > 0
    ]
"""

# ðŸŸ¢ Use input_path variable here
AmazonS3_node1759882748037 = glueContext.create_dynamic_frame.from_options(
    format_options={"quoteChar": "\"", "withHeader": True, "separator": ",", "optimizePerformance": False},
    connection_type="s3",
    format="csv",
    connection_options={"paths": [input_path], "recurse": True},
    transformation_ctx="AmazonS3_node1759882748037"
)

ChangeSchema_node1759883145239 = ApplyMapping.apply(
    frame=AmazonS3_node1759882748037,
    mappings=[
        ("ticket_id", "string", "ticket_id", "string"),
        ("created_at", "string", "created_at", "timestamp"),
        ("resolved_at", "string", "resolved_at", "timestamp"),
        ("agent", "string", "agent", "string"),
        ("priority", "string", "priority", "string"),
        ("num_interactions", "string", "num_interactions", "int"),
        ("issuecat", "string", "issuecat", "string"),
        ("channel", "string", "channel", "string"),
        ("status", "string", "status", "string"),
        ("agent_feedback", "string", "agent_feedback", "string")
    ],
    transformation_ctx="ChangeSchema_node1759883145239"
)

DropNullFields_node1759883353963 = drop_nulls(
    glueContext,
    frame=ChangeSchema_node1759883145239,
    nullStringSet={""},
    nullIntegerSet={},
    transformation_ctx="DropNullFields_node1759883353963"
)

RenameField_node1759883719770 = RenameField.apply(
    frame=DropNullFields_node1759883353963,
    old_name="issuecat",
    new_name="Issue_category",
    transformation_ctx="RenameField_node1759883719770"
)

Filter_node1759883927634 = Filter.apply(
    frame=RenameField_node1759883719770,
    f=lambda row: (row["num_interactions"] >= 0),
    transformation_ctx="Filter_node1759883927634"
)

SqlQuery2786 = '''
select *,
case
    when priority = 'Lw' then 'Low'
    when priority = 'Medum' then 'Medium'
    when priority = 'Hgh' then 'High'
else priority
end as priority
from myDataSource
'''

SQLQuery_node1759884066330 = sparkSqlQuery(
    glueContext,
    query=SqlQuery2786,
    mapping={"myDataSource": Filter_node1759883927634},
    transformation_ctx="SQLQuery_node1759884066330"
)

SelectFields_node1759945617509 = SelectFields.apply(
    frame=SQLQuery_node1759884066330,
    paths=["ticket_id", "created_at", "resolved_at", "agent", "priority", "num_interactions", "Issue_category", "channel", "status"],
    transformation_ctx="SelectFields_node1759945617509"
)

EvaluateDataQuality().process_rows(
    frame=SelectFields_node1759945617509,
    ruleset=DEFAULT_DATA_QUALITY_RULESET,
    publishing_options={
        "dataQualityEvaluationContext": "EvaluateDataQuality_node1759882742974",
        "enableDataQualityResultsPublishing": True
    },
    additional_options={
        "dataQualityResultsPublishing.strategy": "BEST_EFFORT",
        "observations.scope": "ALL"
    }
)

if (SelectFields_node1759945617509.count() >= 1):
    SelectFields_node1759945617509 = SelectFields_node1759945617509.coalesce(1)

AmazonS3_node1759884559732 = glueContext.write_dynamic_frame.from_options(
    frame=SelectFields_node1759945617509,
    connection_type="s3",
    format="glueparquet",
    connection_options={"path": "s3://careplus-data1/support-tickets/processed/", "partitionKeys": []},
    format_options={"compression": "snappy"},
    transformation_ctx="AmazonS3_node1759884559732"
)

job.commit()
